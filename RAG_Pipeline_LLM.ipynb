{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d7117cc",
   "metadata": {},
   "source": [
    "# 🔍 RAG Pipeline For LLMs 🚀\n",
    "\n",
    "\n",
    "## 📖 Project Overview\n",
    "\n",
    "This Notebook implements a **Retrieval-Augmented Generation (RAG)** pipeline using state-of-the-art language models and vector databases. The system intelligently retrieves relevant information from Wikipedia articles and generates accurate, context-aware answers to user questions.\n",
    "\n",
    "\n",
    "\n",
    "### ✨ Key Features\n",
    "\n",
    "- 📚 **Dynamic Knowledge Retrieval** from Wikipedia\n",
    "- 🧮 **Semantic Search** using sentence transformers\n",
    "- ⚡ **Fast Vector Similarity** with FAISS indexing\n",
    "- 🤖 **Intelligent Answer Generation** using pre-trained QA models\n",
    "- 📊 **Confidence Scoring** for answer quality assessment\n",
    "- 🎛️ **Customizable Parameters** (chunk size, retrieval count, etc.)\n",
    "\n",
    "### 🏗️ Architecture\n",
    "\n",
    "```\n",
    "User Query → Embedding → FAISS Search → Retrieve Chunks → QA Model → Answer + Confidence\n",
    "```\n",
    "\n",
    "### 🛠️ Tech Stack & Models\n",
    "\n",
    "**🔧 Core Technologies:**\n",
    "- **🤗 Transformers**: For tokenization and question-answering models\n",
    "- **📝 Sentence Transformers**: For semantic embeddings\n",
    "- **⚡ FAISS**: For efficient vector similarity search\n",
    "- **📖 Wikipedia API**: For knowledge base content\n",
    "- **🐍 Python**: Core implementation language\n",
    "\n",
    "**🤖 AI Models Used:**\n",
    "- **📏 Text Chunking**: `sentence-transformers/all-mpnet-base-v2` tokenizer\n",
    "- **🧮 Vector Embeddings**: `sentence-transformers/all-mpnet-base-v2` (768-dimensional)\n",
    "- **❓ Question Answering**: `deepset/roberta-base-squad2` (RoBERTa fine-tuned on SQuAD 2.0)\n",
    "- **🔍 Vector Search**: FAISS IndexFlatL2 for L2 distance similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eefb66",
   "metadata": {},
   "source": [
    "## 📦 Installation & Setup\n",
    "\n",
    "### 🔧 Installing Required Dependencies\n",
    "\n",
    "First, let's install all the necessary Python packages for our RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ffab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\sandh\\anaconda3\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Requirement already satisfied: transformers in c:\\users\\sandh\\anaconda3\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\sandh\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\sandh\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Collecting tf_keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tf_keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf_keras) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf_keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf_keras) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf_keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sandh\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf_keras) (0.1.0)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.8/1.7 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: tf_keras\n",
      "Successfully installed tf_keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia\n",
    "!pip install transformers\n",
    "!pip install sentence-transformers\n",
    "!pip install faiss-cpu\n",
    "!pip install tf_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ad104",
   "metadata": {},
   "source": [
    "### 📚 Importing Essential Libraries\n",
    "\n",
    "Now let's import all the required libraries for our RAG implementation:\n",
    "\n",
    "- **🔢 NumPy**: For numerical operations and array handling\n",
    "- **📖 Wikipedia**: For fetching knowledge base content\n",
    "- **🤗 Transformers**: For tokenization and QA models  \n",
    "- **📝 Sentence Transformers**: For creating semantic embeddings\n",
    "- **⚡ FAISS**: For efficient vector similarity search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a893dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandh\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wikipedia\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ade958",
   "metadata": {},
   "source": [
    "## 📖 Retrieving Knowledge:\n",
    "\n",
    "Fetch relevant Wikipedia articles based on a given topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99c702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_content(topic):\n",
    "    try: \n",
    "        page = wikipedia.page(topic)\n",
    "        return page.content\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        return None\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        # Handling cases where the topic is ambiguous:\n",
    "        print(f\"Ambiguous topic, Get more specific. Possible options: {e.options}\")\n",
    "        return None\n",
    "\n",
    "# User Input:\n",
    "topic = input(\"Enter a topic to learn about: \")\n",
    "document = get_wikipedia_content(topic)\n",
    "\n",
    "if not document:\n",
    "    print(\"Could not find information on that topic.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1eea6",
   "metadata": {},
   "source": [
    "## ✂️ Text Chunking:\n",
    "\n",
    "Since Wikipedia articles can be long, we will split the text into smaller overlapping chunks for better retrieval:\n",
    "\n",
    "- 📏 **Smart Segmentation**: Breaking text into manageable pieces (256 tokens each)\n",
    "- 🔗 **Overlapping Chunks**: Maintaining context between segments (20 token overlap)\n",
    "- ⚡ **Optimized Processing**: Avoiding token limit warnings and errors\n",
    "- 🎯 **Better Retrieval**: Smaller chunks = more precise similarity matching\n",
    "\n",
    "**Process:** Each chunk represents a focused piece of information that can be independently searched and retrieved for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc0e6f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6573 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 28\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "def split_text(text, chunk_size = 256, chunk_overlap = 20):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunks.append(tokenizer.convert_tokens_to_string(tokens[start:end]))\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - chunk_overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(document)\n",
    "print(f\"Total chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1685e8f",
   "metadata": {},
   "source": [
    "## 🧮 Vector Embeddings & Search Index:\n",
    "\n",
    "### 🔄 Converting Text to Mathematical Vectors\n",
    "\n",
    "Transforms human-readable text into numerical representations that machines can compare:\n",
    "\n",
    "**🎯 The Process:**\n",
    "- 📝 **Sentence Transformers**: Convert each text chunk into 768-dimensional vectors\n",
    "- 🧠 **Semantic Encoding**: Capture meaning, not just keywords\n",
    "- ⚡ **FAISS Indexing**: Store vectors for lightning-fast similarity search\n",
    "- 📊 **L2 Distance**: Measure semantic similarity mathematically\n",
    "\n",
    "To efficiently search for relevant chunks, we will use Sentence Transformers to convert text into embeddings and store them in a FAISS index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a766b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "embeddings = embedding_model.encode(chunks)\n",
    "\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b172d",
   "metadata": {},
   "source": [
    "## 🔍 Smart Query Processing & Retrieval\n",
    "\n",
    "### ❓ From Question to Context\n",
    "\n",
    "In the retrieval phase When a user asks a question, it will:\n",
    "\n",
    "\n",
    "**The Retrieval Pipeline:**\n",
    "\n",
    "1️⃣ **🎯 Query Embedding**: Convert user question to same vector space as knowledge chunks\n",
    "\n",
    "2️⃣ **⚡ Similarity Search**: FAISS finds the k most semantically similar chunks  \n",
    "\n",
    "3️⃣ **📋 Context Assembly**: Gather the most relevant information pieces\n",
    "\n",
    "4️⃣ **🎛️ Customizable Retrieval**: Adjust k (number of chunks) based on needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved chunks: \n",
      "- ( portugal ), reims ( france ) and voros lobogo ( hungary ). the first european cup match took place on 4 september 1955, and ended in a 3 – 3 draw between sporting cp and partizan. the first goal in european cup history was scored by joao baptista martins of sporting cp. the inaugural final took place at the parc des princes between stade de reims and real madrid on 13 june 1956. the spanish squad came back from behind to win 4 – 3 thanks to goals from alfredo di stefano and marquitos, as well as two goals from hector rial. real madrid successfully defended the trophy next season in their home stadium, the santiago bernabeu, against fiorentina. after a scoreless first half, real madrid scored twice in six minutes to defeat the italians. in 1958, milan failed to capitalise after going ahead on the scoreline twice, only for real madrid to equalise. the final, held in heysel stadium, went to extra time where francisco gento scored the game - winning goal to allow real madrid to retain the title for the third consecutive season. in a rematch of the first final, real madrid faced stade reims at the neckarstadion for the 1959 final, and won 2 –\n",
      "- madrid faced stade reims at the neckarstadion for the 1959 final, and won 2 – 0. west german side eintracht frankfurt became the first team not to compete in the latin cup to reach the european cup final. the 1960 final holds the record for the most goals scored, with real madrid beating eintracht frankfurt 7 – 3 at hampden park, courtesy of four goals by ferenc puskas and a hat - trick by alfredo di stefano. this was real madrid ' s fifth consecutive title, a record that still stands today. real madrid ' s reign ended in the 1960 – 61 season when bitter rivals barcelona dethroned them in the first round. barcelona were defeated in the final by portuguese side benfica 3 – 2 at the wankdorf stadium. reinforced by eusebio, benfica defeated real madrid 5 – 3 at the olympic stadium in amsterdam and kept the title for a second consecutive season. benfica wanted to repeat real madrid ' s successful run of the 1950s after reaching the showpiece event of the 1962 – 63 european cup, but a brace from brazilian - italian jose altafini at wembley gave the spoils to milan, making the trophy leave the iberian peninsula for the first time ever. inter milan beat an ageing real madrid\n",
      "- wins ). england has the most winning teams, with six clubs having won the title. the competition has been won by 24 clubs and 13 of them have won it more than once. since the tournament changed name and structure in 1992, only two top - tier football clubs outside the big five european nations ( spain, england, italy, germany and france ) have also reached the final : porto ( 2003 – 04 ) and ajax ( 1994 – 95 and 1995 – 96 ). real madrid is the most successful club in the tournament ' s history, having won it 15 times. madrid is the only club to have won it five times in a row ( the first five editions ). only one club has won all of their matches in a single tournament en route to the tournament victory : bayern munich in the 2019 – 20 season. paris saint - germain are the current european champions, having beaten inter milan 5 – 0 in the 2025 final for their first ever title. = = history = = the first time the champions of two european leagues met was in what was nicknamed the 1895 world championship, when english champions sunderland beat scottish champions heart of midlothian 5 – 3. the first pan - european tournament was the challenge cup, a competition between clubs in the austro - hungarian empire.\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Ask a question about the topic: \")\n",
    "query_embedding = embedding_model.encode([query])\n",
    "query_embedding_array = np.array(query_embedding)\n",
    "\n",
    "k = 3   # Number of chunks to retrieve\n",
    "distances, indices = index.search(query_embedding_array, k)\n",
    "retrieved_chunks = [chunks[i] for i in indices[0]]\n",
    "print(\"Retrieved chunks: \")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(\"- \" + chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e999e1b",
   "metadata": {},
   "source": [
    "## 🤖 Intelligent Answer Generation\n",
    "\n",
    "### 🧠 From Context to Precise Answers\n",
    "\n",
    "The final step where retrieved knowledge becomes intelligent responses:\n",
    "\n",
    "**🎯 Generation Process:**\n",
    "- 🤖 **RoBERTa QA Model**: Pre-trained on SQuAD 2.0 dataset for optimal performance\n",
    "- 📝 **Extractive Answering**: Finds exact answer spans within retrieved context\n",
    "- 🎛️ **Context Fusion**: Combines multiple relevant chunks into unified context\n",
    "- 📊 **Confidence Scoring**: Provides reliability metrics for each answer\n",
    "\n",
    "\n",
    "**🔗 Complete RAG Flow**: `Wikipedia → Chunks → Embeddings → Retrieval → Generation → Answer + Confidence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1088f1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 15\n"
     ]
    }
   ],
   "source": [
    "qa_model_name = \"deepset/roberta-base-squad2\"\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "qa_pipeline = pipeline(\"question-answering\", model=qa_model, tokenizer=qa_tokenizer)\n",
    "\n",
    "context = \" \".join(retrieved_chunks)\n",
    "answer = qa_pipeline(question = query, context = context)\n",
    "print(f\"Answer: {answer['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5da3d",
   "metadata": {},
   "source": [
    "### 📊 Answer Quality Assessment\n",
    "\n",
    "Monitor the reliability and confidence of generated answers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380a4564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Score: 0.3357 (33.6%)\n"
     ]
    }
   ],
   "source": [
    "# Display confidence score\n",
    "print(f\"Confidence Score: {answer['score']:.4f} ({answer['score']*100:.1f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
