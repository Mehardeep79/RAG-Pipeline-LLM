# ğŸ” RAG Pipeline For LLMs ğŸš€


## ğŸ“– Project Overview

A complete **Retrieval-Augmented Generation (RAG)** pipeline implementation using state-of-the-art language models and vector databases. This system intelligently retrieves relevant information from Wikipedia articles and generates accurate, context-aware answers to user questions with confidence scoring.

![RAG Pipeline Architecture](https://img.shields.io/badge/Pipeline-Wikipediaâ†’Chunksâ†’Embeddingsâ†’FAISSâ†’QAâ†’Answer-brightgreen)


## âœ¨ Key Features

- ğŸ“š **Dynamic Knowledge Retrieval** from Wikipedia with error handling
- ğŸ§® **Semantic Search** using sentence transformers (no keyword dependency)
- âš¡ **Fast Vector Similarity** with FAISS indexing (sub-second search)
- ğŸ¤– **Intelligent Answer Generation** using pre-trained QA models
- ğŸ“Š **Confidence Scoring** for answer quality assessment
- ğŸ›ï¸ **Customizable Parameters** (chunk size, retrieval count, overlap)
- âœ‚ï¸ **Smart Text Chunking** with overlapping segments for context preservation

## ğŸ—ï¸ Architecture

```
User Query â†’ Embedding â†’ FAISS Search â†’ Retrieve Chunks â†’ QA Model â†’ Answer + Confidence
     â†“              â†“            â†“              â†“              â†“              â†“
"How many times    [0.2, 0.8,   Find top-k     ["Real Madrid  RoBERTa Model  "15 times"
 has Real Madrid   -0.1, ...]   similar        won 15 times   Processes      (92.3% conf)
 won Champions                   chunks         ..."]          Context
 League?"
```

## ğŸ› ï¸ Tech Stack & Models

**ğŸ”§ Core Technologies:**
- **ğŸ¤— Transformers**: For tokenization and question-answering models
- **ğŸ“ Sentence Transformers**: For semantic embeddings
- **âš¡ FAISS**: For efficient vector similarity search
- **ğŸ“– Wikipedia API**: For knowledge base content
- **ğŸ Python**: Core implementation language

**ğŸ¤– AI Models Used:**
- **ğŸ“ Text Chunking**: `sentence-transformers/all-mpnet-base-v2` tokenizer
- **ğŸ§® Vector Embeddings**: `sentence-transformers/all-mpnet-base-v2` (768-dimensional)
- **â“ Question Answering**: `deepset/roberta-base-squad2` (RoBERTa fine-tuned on SQuAD 2.0) HuggingFace
- **ğŸ” Vector Search**: FAISS IndexFlatL2 for L2 distance similarity

## ğŸš€ Quick Start

### ğŸ“¦ Installation

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/RAG_Pipeline_LLM.git
cd RAG_Pipeline_LLM
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```
**OR**
```bash
pip install wikipedia transformers sentence-transformers faiss-cpu tf_keras numpy
```

3. **Run the Jupyter notebook:**
```bash
jupyter notebook RAG_Pipeline_LLM.ipynb
```

### ğŸ“‹ Usage Guide

1. **Install Dependencies**: Execute the installation cell
2. **Import Libraries**: Run the imports cell
3. **Choose Topic**: Enter any Wikipedia topic when prompted (e.g., "Artificial Intelligence", "Climate Change")
4. **Process Content**: Let the system automatically chunk and embed the content
5. **Ask Questions**: Query the knowledge base with natural language questions
6. **Review Results**: Get answers with confidence scores

### ğŸ’¡ Example Usage

```
Enter a topic to learn about: Artificial Intelligence
Total chunks: 45

Ask a question about the topic: What is machine learning?
Retrieved chunks: [3 most relevant chunks displayed]

Answer: Machine learning is a subset of artificial intelligence...
Confidence Score: 0.8974 (89.7%)
```

## ğŸ›ï¸ Customization Options

The pipeline offers several parameters you can adjust:

- **Chunk Size**: Modify `chunk_size` parameter (default: 256 tokens)
- **Overlap**: Adjust `chunk_overlap` for context preservation (default: 20 tokens)
- **Retrieval Count**: Change `k` value for more/fewer retrieved chunks (default: 3)
- **Models**: Swap embedding or QA models for different performance characteristics

```python
# Example customizations
chunks = split_text(document, chunk_size=512, chunk_overlap=50)  # Larger chunks
k = 5  # Retrieve more chunks for broader context

# Hugging Face model loading examples
from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline

# Load models directly from Hugging Face Hub
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-mpnet-base-v2")
qa_model = AutoModelForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")
qa_pipeline = pipeline("question-answering", model=qa_model, tokenizer=tokenizer)
```

## ğŸ“ Project Structure

```
RAG_Pipeline_LLM/
â”œâ”€â”€ RAG_Pipeline_LLM.ipynb    # Main implementation notebook
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE                   # MIT License
â””â”€â”€ requirements.txt          # Python dependencies
```

## ğŸ”¬ How It Works

### 1ï¸âƒ£ **Knowledge Acquisition**
- Fetches Wikipedia articles on user-specified topics
- Handles disambiguation and missing pages gracefully
- Validates content before processing

### 2ï¸âƒ£ **Text Preprocessing**
- Splits long articles into manageable chunks (256 tokens)
- Maintains context with overlapping segments (20 tokens)
- Optimizes for both retrieval accuracy and processing efficiency

### 3ï¸âƒ£ **Vector Embedding**
- Converts text chunks to 768-dimensional vectors using `all-mpnet-base-v2`
- Captures semantic meaning beyond keyword matching
- Creates searchable mathematical representations

### 4ï¸âƒ£ **Indexing & Search**
- Stores embeddings in FAISS index for fast similarity search
- Uses L2 distance for measuring semantic similarity
- Enables sub-second retrieval from thousands of chunks

### 5ï¸âƒ£ **Answer Generation**
- Uses RoBERTa model fine-tuned on SQuAD 2.0 dataset via Hugging Face Hub
- Leverages `transformers.pipeline()` for streamlined question-answering
- Performs extractive QA (finds answers within context)
- Provides confidence scores for answer reliability
- Automatic model downloading and caching through Hugging Face infrastructure

## ğŸ“Š Performance & Metrics

- **Search Speed**: Sub-second retrieval for 1000+ chunks
- **Accuracy**: High precision with confidence scoring
- **Memory Efficient**: Optimized chunk sizes prevent token overflow
- **Scalable**: Easy to extend to multiple knowledge sources

## ğŸ”® Future Enhancements

- ğŸŒ **Multi-source Knowledge**: Extend beyond Wikipedia to web scraping, PDFs, databases
- ğŸ’¾ **Persistent Storage**: Save embeddings for reuse across sessions
- ğŸ¨ **Web Interface**: Create user-friendly UI with Streamlit/Gradio
- ğŸ“ˆ **Advanced Retrieval**: Implement re-ranking, query expansion, and hybrid search
- ğŸ¤– **Custom Models**: Fine-tune models on domain-specific data
- ğŸ”„ **Real-time Updates**: Dynamic knowledge base updates
- ğŸ“± **API Integration**: REST API for production deployment


## ğŸ“ Contact 

- **ğŸ“§ Email**: sandhu.mehardeep792003@gmail.com
- **ğŸ™ GitHub**: [Mehardeep79](https://github.com/Mehardeep79)
- **ğŸ’¼ LinkedIn**: [Mehardeep Singh Sandhu](https://www.linkedin.com/in/mehardeep-singh-sandhu/)

---

